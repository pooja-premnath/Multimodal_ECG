{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 18:05:58,290 - INFO - Use pytorch device_name: mps\n",
      "2024-10-31 18:05:58,291 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "/Users/sanjaibalajee/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sanjaibalajee/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "2024-10-31 18:06:02,069 - INFO - Model and FAISS index loaded successfully.\n",
      "2024-10-31 18:06:02,070 - INFO - Finding similar text reports for image (FAISS): records100/00000/00004_lr.png\n",
      "2024-10-31 18:06:02,190 - INFO - Finding similar text reports for image (Cosine Similarity): records100/00000/00004_lr.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 similar text reports using FAISS (L2 Distance):\n",
      "ECG ID: 4, Index: 3\n",
      "ECG ID: 624, Index: 611\n",
      "ECG ID: 353, Index: 345\n",
      "ECG ID: 79, Index: 78\n",
      "ECG ID: 317, Index: 309\n",
      "\n",
      "Top 5 similar text reports using Cosine Similarity:\n",
      "ECG ID: 4, Cosine Similarity Score: 0.9998\n",
      "ECG ID: 624, Cosine Similarity Score: 0.9970\n",
      "ECG ID: 353, Cosine Similarity Score: 0.9970\n",
      "ECG ID: 317, Cosine Similarity Score: 0.9969\n",
      "ECG ID: 79, Cosine Similarity Score: 0.9969\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import faiss  # Library for efficient similarity search\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define the folder path for images if needed\n",
    "image_base_folder = \"\"  # Update this to your actual base folder path if needed\n",
    "\n",
    "# Cell 2: Define Model Class and Preprocessing\n",
    "class ImageTextSimilarityModel:\n",
    "    def __init__(self, text_model_name='all-MiniLM-L6-v2', image_model_name='resnet50'):\n",
    "        # Set up device\n",
    "        self.device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "        # Load text and image models\n",
    "        self.text_model = SentenceTransformer(text_model_name)\n",
    "        self.image_model = models.resnet50(pretrained=True)\n",
    "        self.image_model = torch.nn.Sequential(*(list(self.image_model.children())[:-1]))  # Remove last layer\n",
    "        self.image_model.to(self.device)  # Move image model to the specified device\n",
    "\n",
    "        # Other initializations\n",
    "        self.index = None\n",
    "        self.ecg_ids = None\n",
    "        self.average_text_embedding = None  # To use as a proxy for text embeddings in inference\n",
    "\n",
    "    def preprocess_image(self, image_path):\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        return preprocess(image).unsqueeze(0).to(self.device)  # Ensure image tensor is on the same device\n",
    "\n",
    "    # Cell 3: Load Data and Compute Embeddings with Full Relative Path and Logging\n",
    "    def compute_embeddings(self, df):\n",
    "        logging.info(\"Computing text embeddings\")\n",
    "        text_embeddings = self.text_model.encode(df['report'].tolist(), convert_to_tensor=True)\n",
    "        \n",
    "        # Calculate average text embedding for inference proxy\n",
    "        self.average_text_embedding = torch.mean(text_embeddings, dim=0).to(self.device)  # Move to device\n",
    "\n",
    "        logging.info(\"Computing image embeddings\")\n",
    "        image_embeddings = []\n",
    "        for relative_path in df['filename_lr']:  # Use full relative path to construct the image path\n",
    "            image_path = f\"{image_base_folder}{relative_path}.png\"  # Construct full image path with .png\n",
    "            \n",
    "            # Log the image path being processed\n",
    "            logging.info(f\"Processing image: {image_path}\")\n",
    "            \n",
    "            # Process and embed image\n",
    "            image_tensor = self.preprocess_image(image_path)\n",
    "            image_embedding = self.image_model(image_tensor).squeeze().detach()\n",
    "            image_embeddings.append(image_embedding)\n",
    "\n",
    "        # Concatenate image and text embeddings\n",
    "        image_embeddings = torch.stack(image_embeddings)\n",
    "        combined_embeddings = torch.cat((image_embeddings, text_embeddings.to(self.device)), dim=1).cpu().numpy()\n",
    "        \n",
    "        # Set ECG IDs and prepare FAISS index\n",
    "        self.ecg_ids = df['ecg_id'].tolist()\n",
    "        embedding_dim = combined_embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(embedding_dim)\n",
    "        self.index.add(combined_embeddings)\n",
    "\n",
    "    # Cell 4: Save and Load Model\n",
    "    def save_model(self, embeddings_path='faiss_embeddings.pkl', faiss_index_path='faiss_index.bin'):\n",
    "        with open(embeddings_path, 'wb') as f:\n",
    "            pickle.dump({'ecg_ids': self.ecg_ids, 'average_text_embedding': self.average_text_embedding}, f)\n",
    "        faiss.write_index(self.index, faiss_index_path)\n",
    "        logging.info(\"Model and FAISS index saved successfully.\")\n",
    "\n",
    "    def load_model(self, embeddings_path='faiss_embeddings.pkl', faiss_index_path='faiss_index.bin'):\n",
    "        with open(embeddings_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.ecg_ids = data['ecg_ids']\n",
    "            self.average_text_embedding = data['average_text_embedding'].to(self.device)  # Move to device\n",
    "        self.index = faiss.read_index(faiss_index_path)\n",
    "        logging.info(\"Model and FAISS index loaded successfully.\")\n",
    "\n",
    "    # Cell 5: Inference using FAISS for L2 Distance\n",
    "    def find_similar_text_reports_faiss(self, image_relative_path, top_k=5):\n",
    "        image_path = f\"{image_base_folder}{image_relative_path}.png\"  # Construct full image path\n",
    "        logging.info(f\"Finding similar text reports for image (FAISS): {image_path}\")\n",
    "        \n",
    "        # Process and embed the input image\n",
    "        image_tensor = self.preprocess_image(image_path)  # Image tensor already on device\n",
    "        image_embedding = self.image_model(image_tensor).squeeze().detach()\n",
    "        \n",
    "        # Concatenate with average text embedding for inference\n",
    "        input_embedding = torch.cat((image_embedding, self.average_text_embedding.detach())).cpu().numpy()\n",
    "        \n",
    "        # Perform FAISS similarity search\n",
    "        _, indices = self.index.search(np.array([input_embedding]), top_k)\n",
    "        \n",
    "        # Retrieve corresponding ECG IDs\n",
    "        similar_reports = [(self.ecg_ids[i], i) for i in indices[0]]\n",
    "        \n",
    "        return similar_reports\n",
    "\n",
    "    # Cell 6: Inference using Cosine Similarity\n",
    "    def find_similar_text_reports_cosine(self, image_relative_path, top_k=5):\n",
    "        image_path = f\"{image_base_folder}{image_relative_path}.png\"  # Construct full image path\n",
    "        logging.info(f\"Finding similar text reports for image (Cosine Similarity): {image_path}\")\n",
    "        \n",
    "        # Process and embed the input image\n",
    "        image_tensor = self.preprocess_image(image_path)  # Image tensor already on device\n",
    "        image_embedding = self.image_model(image_tensor).squeeze().detach()\n",
    "        \n",
    "        # Concatenate with average text embedding for inference\n",
    "        input_embedding = torch.cat((image_embedding, self.average_text_embedding.detach())).cpu().numpy()\n",
    "        \n",
    "        # Calculate cosine similarity for each embedding in the index\n",
    "        similar_reports = []\n",
    "        for i in range(self.index.ntotal):\n",
    "            candidate_embedding = self.index.reconstruct(i)\n",
    "            similarity_score = cosine_similarity([input_embedding], [candidate_embedding])[0][0]\n",
    "            similar_reports.append((self.ecg_ids[i], similarity_score))\n",
    "        \n",
    "        # Sort by similarity score in descending order and return top_k results\n",
    "        similar_reports = sorted(similar_reports, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        \n",
    "        return similar_reports\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    model = ImageTextSimilarityModel()\n",
    "    df = pd.read_csv('ecg_reports_outpu100_filtered.csv')  # Load your CSV file with data\n",
    "    \n",
    "    # Compute and save embeddings\n",
    "    # model.compute_embeddings(df)\n",
    "    # model.save_model()\n",
    "\n",
    "    # Load model for inference\n",
    "    model.load_model()\n",
    "    image_relative_path = \"records100/00000/00004_lr\"  # Input image relative path without .png extension\n",
    "    \n",
    "    # Find similar reports using FAISS\n",
    "    print(\"Top 5 similar text reports using FAISS (L2 Distance):\")\n",
    "    similar_reports_faiss = model.find_similar_text_reports_faiss(image_relative_path)\n",
    "    for ecg_id, score in similar_reports_faiss:\n",
    "        print(f\"ECG ID: {ecg_id}, Index: {score}\")\n",
    "    \n",
    "    # Find similar reports using Cosine Similarity\n",
    "    print(\"\\nTop 5 similar text reports using Cosine Similarity:\")\n",
    "    similar_reports_cosine = model.find_similar_text_reports_cosine(image_relative_path)\n",
    "    for ecg_id, similarity_score in similar_reports_cosine:\n",
    "        print(f\"ECG ID: {ecg_id}, Cosine Similarity Score: {similarity_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
